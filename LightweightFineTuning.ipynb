{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f35354cd",
      "metadata": {
        "id": "f35354cd"
      },
      "source": [
        "# Lightweight Fine-Tuning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560fb3ff",
      "metadata": {
        "id": "560fb3ff"
      },
      "source": [
        "TODO: In this cell, describe your choices for each of the following\n",
        "\n",
        "* PEFT technique: Low-Rank Adaptation (LoRA)\n",
        "* Model: GPT-2 (gpt2)\n",
        "* Evaluation approach: Accuracy metric with Hugging Face's Trainer\n",
        "* Fine-tuning dataset: Stanford Sentiment Treebank (SST-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8d76bb",
      "metadata": {
        "id": "de8d76bb"
      },
      "source": [
        "## Loading and Evaluating a Foundation Model\n",
        "\n",
        "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll load the pre-trained GPT-2 model and the SST-2 dataset, and evaluate the model's performance prior to fine-tuning."
      ],
      "metadata": {
        "id": "geyCjvDoGlj_"
      },
      "id": "geyCjvDoGlj_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages if needed\n",
        "#!pip install -q transformers datasets evaluate peft torch accelerate"
      ],
      "metadata": {
        "id": "7Ld02vlCEPU3"
      },
      "id": "7Ld02vlCEPU3",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "f28c4a78",
      "metadata": {
        "id": "f28c4a78"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4KaXGtB9Ef1O",
        "outputId": "cd8173bb-023f-48fb-e9ed-e0dae61449d2"
      },
      "id": "4KaXGtB9Ef1O",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SST-2 dataset\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DQHz3JvLEgEX",
        "outputId": "9db123bc-11a6-4496-ecad-a628785fc425"
      },
      "id": "DQHz3JvLEgEX",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'label', 'idx'],\n",
            "        num_rows: 67349\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence', 'label', 'idx'],\n",
            "        num_rows: 872\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'label', 'idx'],\n",
            "        num_rows: 1821\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take sufficient samples for training\n",
        "# Using 10% of the training data (about 6.7K samples) for a more robust training\n",
        "train_size = len(dataset[\"train\"]) // 10\n",
        "eval_size = min(1000, len(dataset[\"validation\"]))  # Up to 1000 samples for evaluation"
      ],
      "metadata": {
        "id": "Loaa-0FXKdBu"
      },
      "id": "Loaa-0FXKdBu",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "019b9f55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "019b9f55",
        "outputId": "4b8634e9-a133-4482-cd33-c3ec4f7421bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 6734\n",
            "Evaluation dataset size: 872\n"
          ]
        }
      ],
      "source": [
        "# Take smaller subsets for faster training and evaluation\n",
        "train_dataset = dataset[\"train\"].select(range(train_size))\n",
        "eval_dataset = dataset[\"validation\"].select(range(eval_size))\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Evaluation dataset size: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "5176b07f",
      "metadata": {
        "id": "5176b07f"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,  # Binary classification (positive/negative)\n",
        "    pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id to match tokenizer\n",
        "    # Properly initialize with good defaults\n",
        "    problem_type=\"single_label_classification\",\n",
        "    return_dict=True\n",
        ")\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qMAlmktVEL1Z",
        "outputId": "41debeb9-41dc-4954-ffa1-5046da7b5a97"
      },
      "id": "qMAlmktVEL1Z",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2ForSequenceClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model size\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Number of trainable parameters: {num_params:,}\")\n",
        "print(f\"Model config:\\n{model.config}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Xg3RXqPZFGRJ",
        "outputId": "9ddaf06a-3df3-4d52-bf4e-4d2981e09466"
      },
      "id": "Xg3RXqPZFGRJ",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: gpt2\n",
            "Number of trainable parameters: 124,441,344\n",
            "Model config:\n",
            "GPT2Config {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)"
      ],
      "metadata": {
        "id": "26SLXLPFFGTo"
      },
      "id": "26SLXLPFFGTo",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5cc15ab3d43e4abbb97b677eff019e9c",
            "71d286ca560741678250de59acc758e8",
            "9c405dca4dcf4c6f90c704a19f91d9c3",
            "040c4edd21164a05b4281587c5eac100",
            "f001537df08548538c699b78b6b55d08",
            "d3db67b2313441f489e61107a796132b",
            "63b571ff42524cbbaed0c87eee5bb8d8",
            "d5c4c921cc7448078532710455a403a1",
            "0ecf7bcd34b142b29e660245adfda1a4",
            "3512c36b461a4345844c9993475e1d4f",
            "f9f2f56f60b445f59654d447247fb138"
          ]
        },
        "id": "GTQcgB4pFGWQ",
        "outputId": "7907ab79-132d-4b7c-ba9b-15ad1eb9a931"
      },
      "id": "GTQcgB4pFGWQ",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6734 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cc15ab3d43e4abbb97b677eff019e9c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "D3qz1Uy6FGY1"
      },
      "id": "D3qz1Uy6FGY1",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define compute metrics function for evaluation\n",
        "accuracy_metric = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "q_BsDl9QEMAI"
      },
      "id": "q_BsDl9QEMAI",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "nFxVHdfWEMDe"
      },
      "id": "nFxVHdfWEMDe",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6a9qnfeyEMOh",
        "outputId": "1a208869-8172-464c-fd0e-e040085843e4"
      },
      "id": "6a9qnfeyEMOh",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-1a073d749371>:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model before fine-tuning\n",
        "print(\"Evaluating the model before fine-tuning...\")\n",
        "base_model_metrics = trainer.evaluate()\n",
        "print(f\"Base model metrics: {base_model_metrics}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "RTXFSOliFSjP",
        "outputId": "2835e8b8-85f3-436d-95f8-699c9fa0133e"
      },
      "id": "RTXFSOliFSjP",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model before fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [55/55 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model metrics: {'eval_loss': 3.072819948196411, 'eval_model_preparation_time': 0.0042, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 6.3659, 'eval_samples_per_second': 136.98, 'eval_steps_per_second': 8.64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0EOW1x7yFSnk"
      },
      "id": "0EOW1x7yFSnk",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tGtVuwwMFSpj"
      },
      "id": "tGtVuwwMFSpj",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Fkk8HtTFSrx"
      },
      "id": "2Fkk8HtTFSrx",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4d52a229",
      "metadata": {
        "id": "4d52a229"
      },
      "source": [
        "## Performing Parameter-Efficient Fine-Tuning\n",
        "\n",
        "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll create a PEFT model using LoRA, train it on our dataset, and save the resulting weights."
      ],
      "metadata": {
        "id": "90EyuuLPGvSf"
      },
      "id": "90EyuuLPGvSf"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "5775fadf",
      "metadata": {
        "id": "5775fadf"
      },
      "outputs": [],
      "source": [
        "# Import PEFT library components\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
        "    r=16,                        # Rank of LoRA matrices\n",
        "    lora_alpha=32,               # Alpha parameter for LoRA scaling\n",
        "    lora_dropout=0.1,            # Dropout probability for LoRA layers\n",
        "    bias=\"none\",                 # Don't adapt bias terms\n",
        "    # Fix: Target the correct GPT-2 attention modules with proper names\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    # tell model to save additional modules.\n",
        "    modules_to_save=[\"classifier\", \"score\"],\n",
        "    # reasoning\n",
        "    inference_mode=True,\n",
        "    # Conv1D\n",
        "    fan_in_fan_out=True,\n",
        ")"
      ],
      "metadata": {
        "id": "mjFIYIWeGaVu"
      },
      "id": "mjFIYIWeGaVu",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PEFT model\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "peft_model.print_trainable_parameters()\n",
        "peft_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vmLZ_bY0GaY-",
        "outputId": "8debc078-f875-4214-821b-66d5a192377c"
      },
      "id": "vmLZ_bY0GaY-",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,536 || all params: 126,064,896 || trainable%: 0.0012\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GPT2ForSequenceClassification(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(50257, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0-11): 12 x GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2Attention(\n",
              "              (c_attn): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=2304, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (c_proj): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=768, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D(nf=3072, nx=768)\n",
              "              (c_proj): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=768, nx=3072)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (score): ModulesToSaveWrapper(\n",
              "        (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
              "        (modules_to_save): ModuleDict(\n",
              "          (default): Linear(in_features=768, out_features=2, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./peft_results\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_ratio=0.1,\n",
        "    report_to=\"none\",\n",
        "    logging_steps=100,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qg2LxmuUGacH",
        "outputId": "f816ef21-314b-4ad3-cf57-6f7d74906ac4"
      },
      "id": "qg2LxmuUGacH",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Yt6SIYbbGafH",
        "outputId": "64153fc7-2ab3-4423-a3f3-b3c98a633aaa"
      },
      "id": "Yt6SIYbbGafH",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-47ca4e0ce565>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "894046c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "894046c0",
        "outputId": "cf171f9e-6a7a-42ee-9f8a-0ca136bae1c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the PEFT model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4210' max='4210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4210/4210 11:37, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.590300</td>\n",
              "      <td>0.581313</td>\n",
              "      <td>0.653670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.579200</td>\n",
              "      <td>0.537459</td>\n",
              "      <td>0.737385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.556100</td>\n",
              "      <td>0.519322</td>\n",
              "      <td>0.754587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.533900</td>\n",
              "      <td>0.494413</td>\n",
              "      <td>0.778670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.551200</td>\n",
              "      <td>0.492145</td>\n",
              "      <td>0.767202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.535600</td>\n",
              "      <td>0.486291</td>\n",
              "      <td>0.771789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.546000</td>\n",
              "      <td>0.482756</td>\n",
              "      <td>0.776376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.520200</td>\n",
              "      <td>0.493989</td>\n",
              "      <td>0.767202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.522900</td>\n",
              "      <td>0.485001</td>\n",
              "      <td>0.774083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.519800</td>\n",
              "      <td>0.478240</td>\n",
              "      <td>0.779817</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4210, training_loss=0.5452583590482589, metrics={'train_runtime': 697.9087, 'train_samples_per_second': 96.488, 'train_steps_per_second': 6.032, 'total_flos': 4482896229826560.0, 'train_loss': 0.5452583590482589, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# Train the model\n",
        "print(\"Training the PEFT model...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the fine-tuned model\n",
        "print(\"Evaluating the fine-tuned model...\")\n",
        "peft_metrics = trainer.evaluate()\n",
        "print(f\"PEFT model metrics: {peft_metrics}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "r0n2AYFOG7uv",
        "outputId": "e8260cb3-7c61-4c59-91e0-4727863f5240"
      },
      "id": "r0n2AYFOG7uv",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the fine-tuned model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [55/55 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PEFT model metrics: {'eval_loss': 0.4782397747039795, 'eval_accuracy': 0.7798165137614679, 'eval_runtime': 7.2774, 'eval_samples_per_second': 119.823, 'eval_steps_per_second': 7.558, 'epoch': 10.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the PEFT model\n",
        "peft_model.save_pretrained(\"./peft_gpt2_sst2\")\n",
        "print(\"PEFT model saved to ./peft_gpt2_sst2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "H_mnqSXuG773",
        "outputId": "0ed72486-92c2-439f-fc05-23cefa92152d"
      },
      "id": "H_mnqSXuG773",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PEFT model saved to ./peft_gpt2_sst2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "c4d4c908",
      "metadata": {
        "id": "c4d4c908"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "b47abf88",
      "metadata": {
        "id": "b47abf88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "fa7fe003",
      "metadata": {
        "id": "fa7fe003"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "615b12c6",
      "metadata": {
        "id": "615b12c6"
      },
      "source": [
        "## Performing Inference with a PEFT Model\n",
        "\n",
        "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll load the saved PEFT model and evaluate its performance compared to the original model."
      ],
      "metadata": {
        "id": "jfhMhWjRH5oz"
      },
      "id": "jfhMhWjRH5oz"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "863ec66e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "863ec66e",
        "outputId": "a0b9838a-c821-45e8-afe2-9f04e81d7733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the base model\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the PEFT model\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_path = \"./peft_gpt2_sst2\"\n",
        "config = PeftConfig.from_pretrained(peft_model_path)\n",
        "print(f\"PEFT config: {config}\")\n",
        "\n",
        "# fix: for reasoning\n",
        "base_model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Creating a fresh base model for PEFT loading...\")\n",
        "inference_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    problem_type=\"single_label_classification\"\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GnIl4bi0H7uT",
        "outputId": "4797c5b0-b1e5-48cf-e40e-149901b71792"
      },
      "id": "GnIl4bi0H7uT",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PEFT config: LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='gpt2', revision=None, inference_mode=True, r=16, target_modules={'c_proj', 'c_attn'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=True, bias='none', use_rslora=False, modules_to_save=['classifier', 'score', 'classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
            "Creating a fresh base model for PEFT loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_loaded = PeftModel.from_pretrained(inference_model, peft_model_path, adapter_name=\"default\").to(device)\n",
        "peft_model_loaded.eval()\n",
        "\n",
        "print(f\"Base model trainable parameters: {sum(p.numel() for p in base_model.parameters() if p.requires_grad)}\")\n",
        "print(f\"PEFT model trainable parameters: {sum(p.numel() for p in peft_model_loaded.parameters() if p.requires_grad)}\")\n",
        "print(f\"PEFT model active adapters: {getattr(peft_model_loaded, 'active_adapters', 'None')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5zzQgtOemdIa",
        "outputId": "bb455522-74f3-417b-dd87-237889767ec1"
      },
      "id": "5zzQgtOemdIa",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model trainable parameters: 124441344\n",
            "PEFT model trainable parameters: 1536\n",
            "PEFT model active adapters: ['default']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run inference on both models with the same inputs\n",
        "def compare_predictions(base_model, peft_model, tokenizer, sample_texts):\n",
        "    \"\"\"Compare predictions from base and PEFT models on sample texts.\"\"\"\n",
        "    base_model.eval()\n",
        "    peft_model.eval()\n",
        "\n",
        "    print(\"Base model parameters:\", sum(p.numel() for p in base_model.parameters() if p.requires_grad))\n",
        "    print(\"PEFT model parameters:\", sum(p.numel() for p in peft_model.parameters() if p.requires_grad))\n",
        "    print(\"PEFT active adapters:\", getattr(peft_model, \"active_adapters\", \"No active adapters property found\"))\n",
        "\n",
        "    for text in sample_texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "        # Get base model prediction\n",
        "        with torch.no_grad():\n",
        "            base_outputs = base_model(**inputs)\n",
        "            base_logits = base_outputs.logits\n",
        "            base_pred = torch.softmax(base_logits, dim=1).tolist()[0]\n",
        "\n",
        "        # Get PEFT model prediction\n",
        "        with torch.no_grad():\n",
        "            peft_outputs = peft_model(**inputs)\n",
        "            peft_logits = peft_outputs.logits\n",
        "            peft_pred = torch.softmax(peft_logits, dim=1).tolist()[0]\n",
        "\n",
        "        # Format results\n",
        "        print(f\"Text: {text}\")\n",
        "        print(f\"Base model prediction - Negative: {base_pred[0]:.4f}, Positive: {base_pred[1]:.4f}\")\n",
        "        print(f\"PEFT model prediction - Negative: {peft_pred[0]:.4f}, Positive: {peft_pred[1]:.4f}\\n\")"
      ],
      "metadata": {
        "id": "BumHatW1H7yU"
      },
      "id": "BumHatW1H7yU",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample texts for inference\n",
        "sample_texts = [\n",
        "    \"This movie was fantastic! I really enjoyed it.\",\n",
        "    \"The acting was terrible and the plot made no sense.\",\n",
        "    \"It was an average film, neither great nor terrible.\",\n",
        "    \"The cinematography was beautiful, but the story was weak.\"\n",
        "]\n",
        "# Compare predictions\n",
        "compare_predictions(base_model, peft_model_loaded, tokenizer, sample_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TIJEfq6wH71B",
        "outputId": "9bfb5726-941f-484e-8057-b9312de60b5c"
      },
      "id": "TIJEfq6wH71B",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model parameters: 124441344\n",
            "PEFT model parameters: 1536\n",
            "PEFT active adapters: ['default']\n",
            "Text: This movie was fantastic! I really enjoyed it.\n",
            "Base model prediction - Negative: 0.0000, Positive: 1.0000\n",
            "PEFT model prediction - Negative: 0.1224, Positive: 0.8776\n",
            "\n",
            "Text: The acting was terrible and the plot made no sense.\n",
            "Base model prediction - Negative: 0.0000, Positive: 1.0000\n",
            "PEFT model prediction - Negative: 0.7578, Positive: 0.2422\n",
            "\n",
            "Text: It was an average film, neither great nor terrible.\n",
            "Base model prediction - Negative: 0.0000, Positive: 1.0000\n",
            "PEFT model prediction - Negative: 0.5514, Positive: 0.4486\n",
            "\n",
            "Text: The cinematography was beautiful, but the story was weak.\n",
            "Base model prediction - Negative: 0.0000, Positive: 1.0000\n",
            "PEFT model prediction - Negative: 0.5730, Positive: 0.4270\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "bc3a8147",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bc3a8147",
        "outputId": "cdb68812-f908-4cdb-d1c8-fff5f3e6acd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-80-233cf829fd1e>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  base_trainer = Trainer(\n",
            "<ipython-input-80-233cf829fd1e>:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  peft_trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "# Set up trainers for both models to evaluate on the test set\n",
        "base_trainer = Trainer(\n",
        "    model=base_model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./base_eval\",\n",
        "        per_device_eval_batch_size=16,\n",
        "        do_train=False,\n",
        "        do_eval=True,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model_loaded,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./peft_eval\",\n",
        "        per_device_eval_batch_size=16,\n",
        "        do_train=False,\n",
        "        do_eval=True,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "bc96905a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "bc96905a",
        "outputId": "5c6c7365-712e-4018-e6e0-7144643961b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating base model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [55/55 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating PEFT model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [55/55 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Evaluate both models\n",
        "print(\"Evaluating base model...\")\n",
        "base_metrics = base_trainer.evaluate()\n",
        "\n",
        "print(\"Evaluating PEFT model...\")\n",
        "peft_metrics = peft_trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare metrics\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(f\"Base model accuracy: {base_metrics['eval_accuracy']:.4f}\")\n",
        "print(f\"PEFT model accuracy: {peft_metrics['eval_accuracy']:.4f}\")\n",
        "print(f\"Improvement: {peft_metrics['eval_accuracy'] - base_metrics['eval_accuracy']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iga_Mrv_IQXm",
        "outputId": "ed647147-c978-4b44-e7a4-c6eb0d5ee15d"
      },
      "id": "iga_Mrv_IQXm",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance Comparison:\n",
            "Base model accuracy: 0.5092\n",
            "PEFT model accuracy: 0.7798\n",
            "Improvement: 0.2706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print PEFT parameter efficiency\n",
        "base_total_params = sum(p.numel() for p in base_model.parameters())\n",
        "base_trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
        "\n",
        "# For PEFT model, count differently\n",
        "peft_total_params = base_total_params\n",
        "peft_trainable_params = sum(p.numel() for p in peft_model_loaded.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nParameter Efficiency:\")\n",
        "print(f\"Base model - Total parameters: {base_total_params:,}\")\n",
        "print(f\"Base model - All parameters would be trained in full fine-tuning\")\n",
        "print(f\"PEFT model - Total parameters: {peft_total_params:,}\")\n",
        "print(f\"PEFT model - Trainable parameters: {peft_trainable_params:,}\")\n",
        "print(f\"Parameter efficiency: Only training {peft_trainable_params / base_total_params:.6%} of the parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eUz7qXYiIQeE",
        "outputId": "1799202a-87de-40a3-ddc2-76f2939ec5cc"
      },
      "id": "eUz7qXYiIQeE",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameter Efficiency:\n",
            "Base model - Total parameters: 124,441,344\n",
            "Base model - All parameters would be trained in full fine-tuning\n",
            "PEFT model - Total parameters: 124,441,344\n",
            "PEFT model - Trainable parameters: 1,536\n",
            "Parameter efficiency: Only training 0.001234% of the parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "866ab28c",
      "metadata": {
        "id": "866ab28c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "f9a32e4e",
      "metadata": {
        "id": "f9a32e4e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5cc15ab3d43e4abbb97b677eff019e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71d286ca560741678250de59acc758e8",
              "IPY_MODEL_9c405dca4dcf4c6f90c704a19f91d9c3",
              "IPY_MODEL_040c4edd21164a05b4281587c5eac100"
            ],
            "layout": "IPY_MODEL_f001537df08548538c699b78b6b55d08"
          }
        },
        "71d286ca560741678250de59acc758e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3db67b2313441f489e61107a796132b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_63b571ff42524cbbaed0c87eee5bb8d8",
            "value": "Map:â€‡100%"
          }
        },
        "9c405dca4dcf4c6f90c704a19f91d9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5c4c921cc7448078532710455a403a1",
            "max": 6734,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ecf7bcd34b142b29e660245adfda1a4",
            "value": 6734
          }
        },
        "040c4edd21164a05b4281587c5eac100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3512c36b461a4345844c9993475e1d4f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f9f2f56f60b445f59654d447247fb138",
            "value": "â€‡6734/6734â€‡[00:01&lt;00:00,â€‡3224.59â€‡examples/s]"
          }
        },
        "f001537df08548538c699b78b6b55d08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3db67b2313441f489e61107a796132b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63b571ff42524cbbaed0c87eee5bb8d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5c4c921cc7448078532710455a403a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ecf7bcd34b142b29e660245adfda1a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3512c36b461a4345844c9993475e1d4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9f2f56f60b445f59654d447247fb138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}