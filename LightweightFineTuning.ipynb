{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yilinmiao/LightweightFineTuning/blob/main/LightweightFineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35354cd",
      "metadata": {
        "id": "f35354cd"
      },
      "source": [
        "# Lightweight Fine-Tuning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560fb3ff",
      "metadata": {
        "id": "560fb3ff"
      },
      "source": [
        "TODO: In this cell, describe your choices for each of the following\n",
        "\n",
        "* PEFT technique: Low-Rank Adaptation (LoRA)\n",
        "* Model: GPT-2 (gpt2)\n",
        "* Evaluation approach: Accuracy metric with Hugging Face's Trainer\n",
        "* Fine-tuning dataset: Stanford Sentiment Treebank (SST-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8d76bb",
      "metadata": {
        "id": "de8d76bb"
      },
      "source": [
        "## Loading and Evaluating a Foundation Model\n",
        "\n",
        "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll load the pre-trained GPT-2 model and the SST-2 dataset, and evaluate the model's performance prior to fine-tuning."
      ],
      "metadata": {
        "id": "geyCjvDoGlj_"
      },
      "id": "geyCjvDoGlj_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages if needed\n",
        "#!pip install -q transformers datasets evaluate peft torch accelerate"
      ],
      "metadata": {
        "id": "7Ld02vlCEPU3"
      },
      "id": "7Ld02vlCEPU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f28c4a78",
      "metadata": {
        "id": "f28c4a78"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "4KaXGtB9Ef1O"
      },
      "id": "4KaXGtB9Ef1O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SST-2 dataset\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "DQHz3JvLEgEX"
      },
      "id": "DQHz3JvLEgEX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take sufficient samples for training\n",
        "# Using 10% of the training data (about 6.7K samples) for a more robust training\n",
        "train_size = len(dataset[\"train\"]) // 10\n",
        "eval_size = min(1000, len(dataset[\"validation\"]))  # Up to 1000 samples for evaluation"
      ],
      "metadata": {
        "id": "Loaa-0FXKdBu"
      },
      "id": "Loaa-0FXKdBu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019b9f55",
      "metadata": {
        "id": "019b9f55"
      },
      "outputs": [],
      "source": [
        "# Take smaller subsets for faster training and evaluation\n",
        "train_dataset = dataset[\"train\"].select(range(train_size))\n",
        "eval_dataset = dataset[\"validation\"].select(range(eval_size))\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Evaluation dataset size: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5176b07f",
      "metadata": {
        "id": "5176b07f"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,  # Binary classification (positive/negative)\n",
        "    pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id to match tokenizer\n",
        "    # Properly initialize with good defaults\n",
        "    problem_type=\"single_label_classification\",\n",
        "    return_dict=True\n",
        ")\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "qMAlmktVEL1Z"
      },
      "id": "qMAlmktVEL1Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model size\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Number of trainable parameters: {num_params:,}\")\n",
        "print(f\"Model config:\\n{model.config}\")"
      ],
      "metadata": {
        "id": "Xg3RXqPZFGRJ"
      },
      "id": "Xg3RXqPZFGRJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)"
      ],
      "metadata": {
        "id": "26SLXLPFFGTo"
      },
      "id": "26SLXLPFFGTo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "GTQcgB4pFGWQ"
      },
      "id": "GTQcgB4pFGWQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "D3qz1Uy6FGY1"
      },
      "id": "D3qz1Uy6FGY1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define compute metrics function for evaluation\n",
        "accuracy_metric = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "q_BsDl9QEMAI"
      },
      "id": "q_BsDl9QEMAI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "nFxVHdfWEMDe"
      },
      "id": "nFxVHdfWEMDe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "6a9qnfeyEMOh"
      },
      "id": "6a9qnfeyEMOh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model before fine-tuning\n",
        "print(\"Evaluating the model before fine-tuning...\")\n",
        "base_model_metrics = trainer.evaluate()\n",
        "print(f\"Base model metrics: {base_model_metrics}\")\n"
      ],
      "metadata": {
        "id": "RTXFSOliFSjP"
      },
      "id": "RTXFSOliFSjP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0EOW1x7yFSnk"
      },
      "id": "0EOW1x7yFSnk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tGtVuwwMFSpj"
      },
      "id": "tGtVuwwMFSpj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Fkk8HtTFSrx"
      },
      "id": "2Fkk8HtTFSrx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4d52a229",
      "metadata": {
        "id": "4d52a229"
      },
      "source": [
        "## Performing Parameter-Efficient Fine-Tuning\n",
        "\n",
        "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll create a PEFT model using LoRA, train it on our dataset, and save the resulting weights."
      ],
      "metadata": {
        "id": "90EyuuLPGvSf"
      },
      "id": "90EyuuLPGvSf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5775fadf",
      "metadata": {
        "id": "5775fadf"
      },
      "outputs": [],
      "source": [
        "# Import PEFT library components\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
        "    r=16,                        # Rank of LoRA matrices\n",
        "    lora_alpha=32,               # Alpha parameter for LoRA scaling\n",
        "    lora_dropout=0.1,            # Dropout probability for LoRA layers\n",
        "    bias=\"none\",                 # Don't adapt bias terms\n",
        "    # Fix: Target the correct GPT-2 attention modules with proper names\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    # tell model to save additional modules.\n",
        "    modules_to_save=[\"classifier\", \"score\"],\n",
        "    # reasoning\n",
        "    inference_mode=True,\n",
        "    # Conv1D\n",
        "    fan_in_fan_out=True,\n",
        ")"
      ],
      "metadata": {
        "id": "mjFIYIWeGaVu"
      },
      "id": "mjFIYIWeGaVu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PEFT model\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "peft_model.print_trainable_parameters()\n",
        "peft_model.to(device)"
      ],
      "metadata": {
        "id": "vmLZ_bY0GaY-"
      },
      "id": "vmLZ_bY0GaY-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./peft_results\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_ratio=0.1,\n",
        "    report_to=\"none\",\n",
        "    logging_steps=100,\n",
        ")"
      ],
      "metadata": {
        "id": "qg2LxmuUGacH"
      },
      "id": "qg2LxmuUGacH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "Yt6SIYbbGafH"
      },
      "id": "Yt6SIYbbGafH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894046c0",
      "metadata": {
        "id": "894046c0"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"Training the PEFT model...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the fine-tuned model\n",
        "print(\"Evaluating the fine-tuned model...\")\n",
        "peft_metrics = trainer.evaluate()\n",
        "print(f\"PEFT model metrics: {peft_metrics}\")"
      ],
      "metadata": {
        "id": "r0n2AYFOG7uv"
      },
      "id": "r0n2AYFOG7uv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the PEFT model\n",
        "peft_model.save_pretrained(\"./peft_gpt2_sst2\")\n",
        "print(\"PEFT model saved to ./peft_gpt2_sst2\")"
      ],
      "metadata": {
        "id": "H_mnqSXuG773"
      },
      "id": "H_mnqSXuG773",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d4c908",
      "metadata": {
        "id": "c4d4c908"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47abf88",
      "metadata": {
        "id": "b47abf88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7fe003",
      "metadata": {
        "id": "fa7fe003"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "615b12c6",
      "metadata": {
        "id": "615b12c6"
      },
      "source": [
        "## Performing Inference with a PEFT Model\n",
        "\n",
        "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll load the saved PEFT model and evaluate its performance compared to the original model."
      ],
      "metadata": {
        "id": "jfhMhWjRH5oz"
      },
      "id": "jfhMhWjRH5oz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863ec66e",
      "metadata": {
        "id": "863ec66e"
      },
      "outputs": [],
      "source": [
        "# Load the base model\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the PEFT model\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_path = \"./peft_gpt2_sst2\"\n",
        "config = PeftConfig.from_pretrained(peft_model_path)\n",
        "print(f\"PEFT config: {config}\")\n",
        "\n",
        "# fix: for reasoning\n",
        "base_model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Creating a fresh base model for PEFT loading...\")\n",
        "inference_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    problem_type=\"single_label_classification\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "GnIl4bi0H7uT"
      },
      "id": "GnIl4bi0H7uT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_loaded = PeftModel.from_pretrained(inference_model, peft_model_path, adapter_name=\"default\").to(device)\n",
        "peft_model_loaded.eval()\n",
        "\n",
        "print(f\"Base model trainable parameters: {sum(p.numel() for p in base_model.parameters() if p.requires_grad)}\")\n",
        "print(f\"PEFT model trainable parameters: {sum(p.numel() for p in peft_model_loaded.parameters() if p.requires_grad)}\")\n",
        "print(f\"PEFT model active adapters: {getattr(peft_model_loaded, 'active_adapters', 'None')}\")"
      ],
      "metadata": {
        "id": "5zzQgtOemdIa"
      },
      "id": "5zzQgtOemdIa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run inference on both models with the same inputs\n",
        "def compare_predictions(base_model, peft_model, tokenizer, sample_texts):\n",
        "    \"\"\"Compare predictions from base and PEFT models on sample texts.\"\"\"\n",
        "    base_model.eval()\n",
        "    peft_model.eval()\n",
        "\n",
        "    print(\"Base model parameters:\", sum(p.numel() for p in base_model.parameters() if p.requires_grad))\n",
        "    print(\"PEFT model parameters:\", sum(p.numel() for p in peft_model.parameters() if p.requires_grad))\n",
        "    print(\"PEFT active adapters:\", getattr(peft_model, \"active_adapters\", \"No active adapters property found\"))\n",
        "\n",
        "    for text in sample_texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "        # Get base model prediction\n",
        "        with torch.no_grad():\n",
        "            base_outputs = base_model(**inputs)\n",
        "            base_logits = base_outputs.logits\n",
        "            base_pred = torch.softmax(base_logits, dim=1).tolist()[0]\n",
        "\n",
        "        # Get PEFT model prediction\n",
        "        with torch.no_grad():\n",
        "            peft_outputs = peft_model(**inputs)\n",
        "            peft_logits = peft_outputs.logits\n",
        "            peft_pred = torch.softmax(peft_logits, dim=1).tolist()[0]\n",
        "\n",
        "        # Format results\n",
        "        print(f\"Text: {text}\")\n",
        "        print(f\"Base model prediction - Negative: {base_pred[0]:.4f}, Positive: {base_pred[1]:.4f}\")\n",
        "        print(f\"PEFT model prediction - Negative: {peft_pred[0]:.4f}, Positive: {peft_pred[1]:.4f}\\n\")"
      ],
      "metadata": {
        "id": "BumHatW1H7yU"
      },
      "id": "BumHatW1H7yU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample texts for inference\n",
        "sample_texts = [\n",
        "    \"This movie was fantastic! I really enjoyed it.\",\n",
        "    \"The acting was terrible and the plot made no sense.\",\n",
        "    \"It was an average film, neither great nor terrible.\",\n",
        "    \"The cinematography was beautiful, but the story was weak.\"\n",
        "]\n",
        "# Compare predictions\n",
        "compare_predictions(base_model, peft_model_loaded, tokenizer, sample_texts)"
      ],
      "metadata": {
        "id": "TIJEfq6wH71B"
      },
      "id": "TIJEfq6wH71B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3a8147",
      "metadata": {
        "id": "bc3a8147"
      },
      "outputs": [],
      "source": [
        "# Set up trainers for both models to evaluate on the test set\n",
        "base_trainer = Trainer(\n",
        "    model=base_model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./base_eval\",\n",
        "        per_device_eval_batch_size=16,\n",
        "        do_train=False,\n",
        "        do_eval=True,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model_loaded,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./peft_eval\",\n",
        "        per_device_eval_batch_size=16,\n",
        "        do_train=False,\n",
        "        do_eval=True,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc96905a",
      "metadata": {
        "id": "bc96905a"
      },
      "outputs": [],
      "source": [
        "# Evaluate both models\n",
        "print(\"Evaluating base model...\")\n",
        "base_metrics = base_trainer.evaluate()\n",
        "\n",
        "print(\"Evaluating PEFT model...\")\n",
        "peft_metrics = peft_trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare metrics\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(f\"Base model accuracy: {base_metrics['eval_accuracy']:.4f}\")\n",
        "print(f\"PEFT model accuracy: {peft_metrics['eval_accuracy']:.4f}\")\n",
        "print(f\"Improvement: {peft_metrics['eval_accuracy'] - base_metrics['eval_accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "iga_Mrv_IQXm"
      },
      "id": "iga_Mrv_IQXm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print PEFT parameter efficiency\n",
        "base_total_params = sum(p.numel() for p in base_model.parameters())\n",
        "base_trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
        "\n",
        "# For PEFT model, count differently\n",
        "peft_total_params = base_total_params\n",
        "peft_trainable_params = sum(p.numel() for p in peft_model_loaded.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nParameter Efficiency:\")\n",
        "print(f\"Base model - Total parameters: {base_total_params:,}\")\n",
        "print(f\"Base model - All parameters would be trained in full fine-tuning\")\n",
        "print(f\"PEFT model - Total parameters: {peft_total_params:,}\")\n",
        "print(f\"PEFT model - Trainable parameters: {peft_trainable_params:,}\")\n",
        "print(f\"Parameter efficiency: Only training {peft_trainable_params / base_total_params:.6%} of the parameters\")"
      ],
      "metadata": {
        "id": "eUz7qXYiIQeE"
      },
      "id": "eUz7qXYiIQeE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "866ab28c",
      "metadata": {
        "id": "866ab28c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a32e4e",
      "metadata": {
        "id": "f9a32e4e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}